{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-Coder-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34c051933674ddea9cb752f707b32c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1 samples per task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solutions:   0%|          | 0/164 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   1%|          | 1/164 [00:17<48:17, 17.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   1%|          | 2/164 [00:27<35:55, 13.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   2%|▏         | 3/164 [00:51<48:51, 18.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   2%|▏         | 4/164 [00:57<35:00, 13.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   3%|▎         | 5/164 [00:58<23:10,  8.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   4%|▎         | 6/164 [00:59<16:14,  6.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   4%|▍         | 7/164 [01:00<11:20,  4.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   5%|▍         | 8/164 [01:26<29:13, 11.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   5%|▌         | 9/164 [01:56<44:37, 17.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   6%|▌         | 10/164 [02:42<1:07:19, 26.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   7%|▋         | 11/164 [02:45<48:30, 19.02s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   7%|▋         | 12/164 [03:35<1:11:47, 28.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   8%|▊         | 13/164 [03:38<52:07, 20.71s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   9%|▊         | 14/164 [04:28<1:13:37, 29.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:   9%|▉         | 15/164 [05:17<1:28:14, 35.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  10%|▉         | 16/164 [06:07<1:38:10, 39.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  10%|█         | 17/164 [06:56<1:44:45, 42.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  11%|█         | 18/164 [07:16<1:27:13, 35.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  12%|█▏        | 19/164 [08:06<1:36:37, 39.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  12%|█▏        | 20/164 [08:56<1:42:55, 42.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  13%|█▎        | 21/164 [09:45<1:47:06, 44.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  13%|█▎        | 22/164 [10:35<1:49:44, 46.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  14%|█▍        | 23/164 [10:37<1:17:28, 32.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  15%|█▍        | 24/164 [11:26<1:28:35, 37.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  15%|█▌        | 25/164 [12:16<1:36:06, 41.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  16%|█▌        | 26/164 [12:19<1:08:49, 29.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  16%|█▋        | 27/164 [13:09<1:21:53, 35.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  17%|█▋        | 28/164 [13:58<1:30:38, 39.99s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  18%|█▊        | 29/164 [14:48<1:36:29, 42.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  18%|█▊        | 30/164 [15:15<1:25:24, 38.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating solutions:  19%|█▉        | 31/164 [15:36<1:13:16, 33.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from human_eval.data import write_jsonl, read_problems\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from human_eval.evaluation import evaluate_functional_correctness\n",
    "from molm.utils.utils import extract_generation_code, languge_settings\n",
    "\n",
    "\n",
    "\n",
    "def build_qwen_instruction(question: str, languge: str = 'python', ):\n",
    "    return \"\"\"\n",
    "Please continue to complete the function. You are not allowed to modify the given code and do the completion only. Please return all completed function in a codeblock. Here is the given code to do completion:\n",
    "```{}\n",
    "{}\n",
    "```\n",
    "\"\"\".strip().format(\n",
    "        languge.lower(), question.strip()\n",
    "    )\n",
    "\n",
    "def generate_one_completion(question, model, tokenizer, max_new_tokens=1024):\n",
    "    \"\"\"Generate a single completion for a given prompt\"\"\"\n",
    "    prompt = build_qwen_instruction(question)\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    \n",
    "    completion = tokenizer.decode(outputs[0][len(inputs[0]) :], skip_special_tokens=True)\n",
    "    # code = extract_generation_code(completion, lang_code='python', verbose=True)\n",
    "    return completion\n",
    "\n",
    "def main():\n",
    "    model_name = \"Qwen/Qwen2.5-Coder-7B\"  \n",
    "    print(f\"Loading {model_name}...\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    problems = read_problems()\n",
    "    num_samples_per_task = 1  # Increase this for pass@k metrics\n",
    "    \n",
    "    print(f\"Generating {num_samples_per_task} samples per task...\")\n",
    "    samples = []\n",
    "    \n",
    "    for task_id in tqdm(problems.keys(), desc=\"Generating solutions\"):\n",
    "        for _ in range(num_samples_per_task):\n",
    "            try:\n",
    "                completion = generate_one_completion(problems[task_id][\"prompt\"], model, tokenizer)\n",
    "                samples.append({\n",
    "                    \"task_id\": task_id,\n",
    "                    \"completion\": completion\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError on task {task_id}: {str(e)}\")\n",
    "    \n",
    "    output_file = f\"samples_{model_name.replace('/', '_')}.json\"\n",
    "    write_jsonl(output_file, samples)\n",
    "    print(f\"Samples saved to {output_file}\")\n",
    "    print(\"\\nTo evaluate, run:\")\n",
    "    result = evaluate_functional_correctness(\n",
    "        sample_file=output_file,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moelm-djswBcic-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
