{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from typing import Dict, Any, Optional\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    model_path: str\n",
    "    tokenizer_path: str\n",
    "    max_length: int = 512\n",
    "    temperature: float = 0.7\n",
    "    \n",
    "class ModelRouter:\n",
    "    def __init__(self, classifier_path: str = \"prompt_classifier\"):\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.classifier = BertForSequenceClassification.from_pretrained(classifier_path)\n",
    "        self.classifier_tokenizer = BertTokenizer.from_pretrained(classifier_path)\n",
    "        self.classifier.to(self.device)\n",
    "        self.classifier.eval()\n",
    "        \n",
    "        self.model_configs = {\n",
    "            \"GSM8K\": ModelConfig(\n",
    "                name=\"GSM8K Solver\",\n",
    "                model_path=\"meta-math/MetaMath-Mistral-7B\",  \n",
    "                tokenizer_path=\"meta-math/MetaMath-Mistral-7B\"\n",
    "            ),\n",
    "            \"HumanEval\": ModelConfig(\n",
    "                name=\"Code Generator\",\n",
    "                model_path=\"Qwen/Qwen2.5-Coder-7B\",  \n",
    "                tokenizer_path=\"Qwen/Qwen2.5-Coder-7B\",\n",
    "                max_length=1024\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.models: Dict[str, Any] = {}\n",
    "        self.tokenizers: Dict[str, Any] = {}\n",
    "        self.load_all_models()\n",
    "        \n",
    "    def load_all_models(self) -> None:\n",
    "        for category, config in self.model_configs.items():\n",
    "            print(f\"Loading {category} model...\")\n",
    "            self.models[category] = AutoModelForCausalLM.from_pretrained(config.model_path)\n",
    "            self.tokenizers[category] = AutoTokenizer.from_pretrained(config.tokenizer_path)\n",
    "            self.models[category].to(self.device)\n",
    "            self.models[category].eval()\n",
    "    \n",
    "        \n",
    "    def classify_prompt(self, prompt: str) -> dict:\n",
    "        encoding = self.classifier_tokenizer(\n",
    "            prompt,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.classifier(input_ids, attention_mask=attention_mask)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "            prediction = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][prediction].item()\n",
    "        \n",
    "        dataset_mapping = {0: \"GSM8K\", 1: \"HumanEval\"}\n",
    "        return {\n",
    "            \"category\": dataset_mapping[prediction],\n",
    "            \"confidence\": confidence,\n",
    "            \"probabilities\": {\n",
    "                \"GSM8K\": probabilities[0][0].item(),\n",
    "                \"HumanEval\": probabilities[0][1].item()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_response(self, prompt: str, category: str) -> str:\n",
    "        config = self.model_configs[category]\n",
    "        model = self.models[category]\n",
    "        tokenizer = self.tokenizers[category]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            max_length=config.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=config.max_length,\n",
    "                temperature=config.temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def process_prompt(self, prompt: str, override_category: Optional[str] = None) -> dict:\n",
    "        if override_category:\n",
    "            category = override_category\n",
    "            classification = {\"category\": category, \"confidence\": 1.0}\n",
    "        else:\n",
    "            classification = self.classify_prompt(prompt)\n",
    "            category = classification[\"category\"]\n",
    "        \n",
    "        response = self.generate_response(prompt, category)\n",
    "        \n",
    "        return {\n",
    "            \"classification\": classification,\n",
    "            \"category\": category,\n",
    "            \"response\": response,\n",
    "            \"model_used\": self.model_configs[category].name\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    router = ModelRouter()\n",
    "    \n",
    "    math_prompt = \"If John has 5 apples and gives 2 to Mary, how many does he have left?\"\n",
    "    code_prompt = \"\"\"Write a function that takes two lists and returns their intersection.\"\"\"\n",
    "    \n",
    "    math_result = router.process_prompt(math_prompt)\n",
    "    code_result = router.process_prompt(code_prompt)\n",
    "    \n",
    "    print(\"\\nMath Problem:\")\n",
    "    print(f\"Classification: {math_result['classification']}\")\n",
    "    print(f\"Response: {math_result['response']}\")\n",
    "    \n",
    "    print(\"\\nCoding Problem:\")\n",
    "    print(f\"Classification: {code_result['classification']}\")\n",
    "    print(f\"Response: {code_result['response']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moelm-djswBcic-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
